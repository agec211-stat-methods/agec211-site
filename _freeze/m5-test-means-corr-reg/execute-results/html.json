{
  "hash": "26fe4dcb76e36cc5147ab335f78dc166",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Test on means, correlations and regression analysis\"\n---\n\n\n\n\n1.  Introduction to hypothesis testing\n2.  Parametric vs non-parametric tests\n3.  Independent samples t-test\n4.  Paired samples t-test\n5.  One-way ANOVA\n6.  Two-way ANOVA\n7.  Chi-square test\n8.  Correlation\n9.  Regression analysis\n\n<a href=\"https://agec211-stat-methods.github.io/agec211-lecture/module-5-efa\" target=\"_blank\" class=\"btn-slides\">View slides in new window</a>\n\n::: {.box style=\"padding-bottom:56.25%; position:relative; display:block; width: 100%\"}\n<iframe width=\"100%\" height=\"100%\" src=\"https://agec211-stat-methods.github.io/agec211-lecture/module-5-efa\" frameborder=\"0\" allowfullscreen style=\"position:absolute; top:0; left: 0\">\n\n</iframe>\n:::\n\n:::: callout-tip\n## Presentation keyboard shortcuts\n\n::: text-line-space\n<ul>\n\n<li>Use <kbd>←</kbd> and <kbd>→</kbd> to navigate through the slides</li>\n\n<li>Use <kbd>F</kbd> to toggle full screen</li>\n\n<li>Use <kbd>O</kbd> to view an overview of all slides</li>\n\n<li>Use <kbd>?</kbd> to see a list of other shortcuts</li>\n\n</ul>\n:::\n::::\n\n\n# Class demonstration\n\n# Indepedent t-test\n\nUsing the mpg dataset, test whether there is a statistically significant difference in the hwy miles per gallon (hwy) between 4-cylinder and 8-cylinder automobiles.\n\nStep 1: load the dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# libraries\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggstatsplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(report)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'report' was built under R version 4.4.2\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nmpg_dta <- ggplot2::mpg |> \n  filter(cyl %in% c(4, 8)) |> \n  mutate(cyl = as.factor(cyl))\n```\n:::\n\n\n\n\nStep 2: Normality test\n\n-   Ho: The data is normally distributed\n-   Ha: The data is not normally distributed\n\nResult: The data is not normally distributed Decision: Reject Ho and use a non-parametric tests (Mann-Whitney U test)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# normality test\nmpg_dta |> \n  group_by(cyl) |> \n  summarise(\n    shapiro_test = shapiro.test(hwy)$statistic,\n    p_value = shapiro.test(hwy)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  cyl   shapiro_test   p_value\n  <fct>        <dbl>     <dbl>\n1 4            0.919 0.0000756\n2 8            0.924 0.000390 \n```\n\n\n:::\n:::\n\n\n\n\nStep 3: test for homogeneity of variance\n\n-   Ho: The data has equal variance\n-   Ha: The data does not have equal variance\n\nDecision: using 5% level of significance, we fail to reject the null hypothesis and conclude that the data has equal variance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# homogeneity of variance\n\nleveneTest(data = mpg_dta, hwy ~ cyl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(>F)  \ngroup   1   3.277 0.07227 .\n      149                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nStep 4: Independent t-test\n\nResult: t = 17.186, df = 149, p-value \\< 0.05\n\nDecision rule: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the hwy miles per gallon (hwy) between 4-cylinder and 8-cylinder automobiles.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# independent t-test\nt.test(data = mpg_dta, hwy ~ cyl, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  hwy by cyl\nt = 17.186, df = 149, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 4 and group 8 is not equal to 0\n95 percent confidence interval:\n  9.889126 12.458669\nsample estimates:\nmean in group 4 mean in group 8 \n       28.80247        17.62857 \n```\n\n\n:::\n:::\n\n\n\n\nIn case, variance are not equal, we can use the Welch's t-test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# independent t-test\nt.test(data = mpg_dta, hwy ~ cyl, var.equal = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  hwy by cyl\nt = 17.586, df = 144.65, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 4 and group 8 is not equal to 0\n95 percent confidence interval:\n  9.918065 12.429731\nsample estimates:\nmean in group 4 mean in group 8 \n       28.80247        17.62857 \n```\n\n\n:::\n:::\n\n\n\n\nStep 5: Visualization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualization\nggbetweenstats(\n  data = mpg_dta,\n  x = cyl,\n  y = hwy,\n  title = \"Highway miles per gallon by cylinder\",\n  xlab = \"Cylinder\",\n  ylab = \"Highway miles per gallon\"\n)\n```\n\n::: {.cell-output-display}\n![](m5-test-means-corr-reg_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n## Mann-whitney U test\n\nResults: W = 5569.5, p-value \\< 0.05\n\nDecision rule: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the hwy miles per gallon (hwy) between 4-cylinder and 8-cylinder automobiles.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mann-Whitney U test\nwilcox.test(data = mpg_dta, hwy ~ cyl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  hwy by cyl\nW = 5569.5, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\n\n\n# Dependent sample t-test\n\nUsing the a synthetic data on training scores of before and after a training program, test whether there is a statistically significant difference in the scores before and after the training program.\n\nStep 1: load the dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## using a synthetic dataset\nbefore <- c(12.2, 14.6, 13.4, 11.2, 12.7, 10.4, 15.8, 13.9, 9.5, 14.2)\nafter <- c(13.5, 15.2, 13.6, 12.8, 13.7, 11.3, 16.5, 13.4, 8.7, 14.6)\n\ntraining_data <- tibble(subject = rep(c(1:10), 2), \n                   time = rep(c(\"before\", \"after\"), each = 10),\n                   score = c(before, after)) |> \n  mutate(time = as_factor(time))\n\ntraining_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 3\n   subject time   score\n     <int> <fct>  <dbl>\n 1       1 before  12.2\n 2       2 before  14.6\n 3       3 before  13.4\n 4       4 before  11.2\n 5       5 before  12.7\n 6       6 before  10.4\n 7       7 before  15.8\n 8       8 before  13.9\n 9       9 before   9.5\n10      10 before  14.2\n11       1 after   13.5\n12       2 after   15.2\n13       3 after   13.6\n14       4 after   12.8\n15       5 after   13.7\n16       6 after   11.3\n17       7 after   16.5\n18       8 after   13.4\n19       9 after    8.7\n20      10 after   14.6\n```\n\n\n:::\n:::\n\n\n\n\nStep 2: Normality test\n\n-   Ho: The data is normally distributed\n-   Ha: The data is not normally distributed\n\nResults: before and after test scores have p-values \\> 0.05. Decision: Fail to reject Ho and conclude that the data is normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# normality test\ntraining_data |> \n  group_by(time) |> \n  summarise(\n    shapiro_test = shapiro.test(score)$statistic,\n    p_value = shapiro.test(score)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  time   shapiro_test p_value\n  <fct>         <dbl>   <dbl>\n1 before        0.977   0.945\n2 after         0.927   0.423\n```\n\n\n:::\n:::\n\n\n\n\nStep 3: Homogeneity of variance\n\n-   Ho: The data has equal variance\n-   Ha: The data does not have equal variance\n\nResults: p-value \\> 0.05 Decision: Fail to reject Ho and conclude that the data has equal variance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# homogeneity of variance\nleveneTest(data = training_data, score ~ time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.1086 0.7455\n      18               \n```\n\n\n:::\n:::\n\n\n\n\nStep 4: Dependent sample t-test\n\nResult: t = -2.272, p-value \\< 0.05\n\nDecision: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the scores before and after the training program.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dependent sample t-test\n# t.test(data = training_data, score ~ time, paired = TRUE)\n\nt.test(training_data$score[training_data$time == \"before\"],\n       training_data$score[training_data$time == \"after\"],\n       paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  training_data$score[training_data$time == \"before\"] and training_data$score[training_data$time == \"after\"]\nt = -2.272, df = 9, p-value = 0.0492\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.077655745 -0.002344255\nsample estimates:\nmean difference \n          -0.54 \n```\n\n\n:::\n:::\n\n\n\n\nStep 5: visualization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualization\nggwithinstats(\n  data = training_data,\n  x = time,\n  y = score,\n  title = \"Training scores before and after\",\n  xlab = \"Time\",\n  ylab = \"Scores\"\n)\n```\n\n::: {.cell-output-display}\n![](m5-test-means-corr-reg_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n# One-way ANOVA\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\n# Create the dataframe\nexam_data <- data.frame(\n  group = c(\n    rep(1, 10),\n    rep(2, 10),\n    rep(3, 10)\n  ),\n  exam = c(\n    50, 45, 48, 47, 45, 49, 50, 54, 57, 55, # Group 1\n    63, 55, 54, 49, 65, 46, 53, 67, 58, 50, # Group 2\n    71, 67, 68, 62, 65, 58, 63, 69, 70, 61  # Group 3\n  )\n) |> \n  mutate(group = as_factor(group))\n\nexam_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   group exam\n1      1   50\n2      1   45\n3      1   48\n4      1   47\n5      1   45\n6      1   49\n7      1   50\n8      1   54\n9      1   57\n10     1   55\n11     2   63\n12     2   55\n13     2   54\n14     2   49\n15     2   65\n16     2   46\n17     2   53\n18     2   67\n19     2   58\n20     2   50\n21     3   71\n22     3   67\n23     3   68\n24     3   62\n25     3   65\n26     3   58\n27     3   63\n28     3   69\n29     3   70\n30     3   61\n```\n\n\n:::\n:::\n\n\n\n\nStep 2: Normality test\n\n-   Ho: The data is normally distributed\n-   Ha: The data is not normally distributed\n\nResults: p-values \\> 0.05\n\nDecision: Fail to reject Ho and conclude that the data is normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# normality test\nexam_data |> \n  group_by(group) |> \n  summarise(\n    shapiro_test = shapiro.test(exam)$statistic,\n    p_value = shapiro.test(exam)$p.value\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  group shapiro_test p_value\n  <fct>        <dbl>   <dbl>\n1 1            0.929   0.437\n2 2            0.949   0.652\n3 3            0.956   0.741\n```\n\n\n:::\n:::\n\n\n\n\nStep 3: Homogeneity of variance\n\n-   Ho: The data has equal variance\n-   Ha: The data does not have equal variance\n\nResults: p-value \\> 0.05\n\nDecision: Fail to reject Ho and conclude that the data has equal variance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# homogeneity of variance\nleveneTest(data = exam_data, exam ~ group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2  1.7343 0.1956\n      27               \n```\n\n\n:::\n:::\n\n\n\n\nStep 4: One-way ANOVA\n\nResults: F = 21.01, p-value \\< 0.05\n\nDecision: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the exam scores between the three groups.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# one-way ANOVA\naov_result <- aov(data = exam_data, exam ~ group)\n\nsummary(aov_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ngroup        2 1205.1   602.5   21.01 3.15e-06 ***\nResiduals   27  774.4    28.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreport(aov_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe ANOVA (formula: exam ~ group) suggests that:\n\n  - The main effect of group is statistically significant and large (F(2, 27) =\n21.01, p < .001; Eta2 = 0.61, 95% CI [0.39, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n```\n\n\n:::\n:::\n\n\n\n\nStep 5: Post-hoc\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# post-hoc\nposthoc <- TukeyHSD(aov_result)\n\nposthoc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = exam ~ group, data = exam_data)\n\n$group\n    diff        lwr      upr     p adj\n2-1  6.0 0.06165421 11.93835 0.0472996\n3-1 15.4 9.46165421 21.33835 0.0000020\n3-2  9.4 3.46165421 15.33835 0.0015175\n```\n\n\n:::\n:::\n\n\n\n\nStep 6: Visualization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualization\nggbetweenstats(\n  data = exam_data,\n  x = group,\n  y = exam,\n  title = \"Exam scores by group\",\n  xlab = \"Group\",\n  ylab = \"Exam scores\"\n)\n```\n\n::: {.cell-output-display}\n![](m5-test-means-corr-reg_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n# Kruskal-Wallis\n\nIf data is not normally distributed, we can use the Kruskal-Wallis test.\n\nResults: p-value \\< 0.05\n\nDecision: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the exam scores between the three groups.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Kruskal-Wallis test\nkruskal.test(data = exam_data, exam ~ group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  exam by group\nKruskal-Wallis chi-squared = 17.086, df = 2, p-value = 0.0001949\n```\n\n\n:::\n:::\n\n\n\n\n## Post-hoc test\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# post-hoc\npairwise.wilcox.test(exam_data$exam, exam_data$group, p.adjust.method = \"BH\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wilcox.test.default(xi, xj, paired = paired, ...): cannot compute\nexact p-value with ties\nWarning in wilcox.test.default(xi, xj, paired = paired, ...): cannot compute\nexact p-value with ties\nWarning in wilcox.test.default(xi, xj, paired = paired, ...): cannot compute\nexact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  exam_data$exam and exam_data$group \n\n  1       2      \n2 0.05802 -      \n3 0.00054 0.01080\n\nP value adjustment method: BH \n```\n\n\n:::\n:::\n\n\n\n\n# Correlation\n\n## Pearson correlation\n\nTest if there is a relationship between mpg and car weight using mtcars dataset.\n\nStep 1: load the data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nmtcars_data <- mtcars\n\nmtcars_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n```\n\n\n:::\n:::\n\n\n\n\nStep 2: Normality test\n\nResults: both wt and mpg have p-values \\> 0.05\n\nDecision: Fail to reject Ho and conclude that the data is normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# normality test for mpg\nshapiro.test(mtcars_data$mpg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  mtcars_data$mpg\nW = 0.94756, p-value = 0.1229\n```\n\n\n:::\n\n```{.r .cell-code}\n# normality test for wt\nshapiro.test(mtcars_data$wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  mtcars_data$wt\nW = 0.94326, p-value = 0.09265\n```\n\n\n:::\n:::\n\n\n\n\nStep 3: Pearson correlation\n\nResults: Correlation coefficient = -0.867, p-value \\< 0.05\n\nInterpretation: There is a strong negative correlation/association between mpg and wt. As the weight of the car increases, the miles per gallon decreases.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pearson correlation\ncor.test(mtcars_data$mpg, mtcars_data$wt, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  mtcars_data$mpg and mtcars_data$wt\nt = -9.559, df = 30, p-value = 1.294e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9338264 -0.7440872\nsample estimates:\n       cor \n-0.8676594 \n```\n\n\n:::\n:::\n\n\n\n\n## Spearman correlation\n\nAssuming the data is not normally distributed, we can use the Spearman correlation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Spearman correlation\ncor.test(mtcars_data$mpg, mtcars_data$wt, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cor.test.default(mtcars_data$mpg, mtcars_data$wt, method =\n\"spearman\"): Cannot compute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tSpearman's rank correlation rho\n\ndata:  mtcars_data$mpg and mtcars_data$wt\nS = 10292, p-value = 1.488e-11\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n-0.886422 \n```\n\n\n:::\n:::\n\n\n\n\n# Regression analysis\n\n## Simple linear regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\n\n## import synthetic data\ndf <- tibble(\n  Week = 1:15,\n  Pie_Sales = c(350, 460, 350, 430, 350, 380, 430, 470, 450, 490, 340, 300, 440, 450, 300),\n  Price = c(5.50, 7.50, 8.00, 8.00, 6.80, 7.50, 4.50, 6.40, 7.00, 5.00, 7.20, 7.90, 5.90, 5.00, 7.00),\n  Advertising = c(3.3, 3.3, 3.0, 4.5, 3.0, 4.0, 3.0, 3.7, 3.5, 4.0, 3.5, 3.2, 4.0, 3.5, 2.7)\n)\n\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 4\n    Week Pie_Sales Price Advertising\n   <int>     <dbl> <dbl>       <dbl>\n 1     1       350   5.5         3.3\n 2     2       460   7.5         3.3\n 3     3       350   8           3  \n 4     4       430   8           4.5\n 5     5       350   6.8         3  \n 6     6       380   7.5         4  \n 7     7       430   4.5         3  \n 8     8       470   6.4         3.7\n 9     9       450   7           3.5\n10    10       490   5           4  \n11    11       340   7.2         3.5\n12    12       300   7.9         3.2\n13    13       440   5.9         4  \n14    14       450   5           3.5\n15    15       300   7           2.7\n```\n\n\n:::\n:::\n\n\n\n\nModel: Pie sales = a + b Price\n\n$$\n\\text{pie sales} = \\beta_0 - \\beta_1 Price\n$$\n\n$$\n\\text{pie sales} = 558.28 - 24.03 Price\n$$\n\nInterpretation: for every 1 peso increase in Price, pie sales on average will decrease by 24.03 pies per week, holding other factors constant cet. par.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simple linear regression\nlm(data = df, Pie_Sales ~ Price) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Pie_Sales ~ Price, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-90.040 -45.040   1.977  55.926  81.977 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   558.28      90.44   6.173 3.36e-05 ***\nPrice         -24.03      13.48  -1.783   0.0979 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.09 on 13 degrees of freedom\nMultiple R-squared:  0.1965,\tAdjusted R-squared:  0.1347 \nF-statistic: 3.179 on 1 and 13 DF,  p-value: 0.09794\n```\n\n\n:::\n:::\n\n\n\n\n## multiple linear regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simple linear regression\nlm(data = df, Pie_Sales ~ Price + Advertising) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Pie_Sales ~ Price + Advertising, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.795 -33.796  -9.088  17.175  96.155 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   306.53     114.25   2.683   0.0199 *\nPrice         -24.98      10.83  -2.306   0.0398 *\nAdvertising    74.13      25.97   2.855   0.0145 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.46 on 12 degrees of freedom\nMultiple R-squared:  0.5215,\tAdjusted R-squared:  0.4417 \nF-statistic: 6.539 on 2 and 12 DF,  p-value: 0.01201\n```\n\n\n:::\n:::\n",
    "supporting": [
      "m5-test-means-corr-reg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}