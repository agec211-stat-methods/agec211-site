---
title: "Module 5: Test on mean, correlation and regression analysis"
format: html
editor: visual
---

# Indepedent t-test

Using the mpg dataset, test whether there is a statistically significant difference in the hwy miles per gallon (hwy) between 4-cylinder and 8-cylinder automobiles.

Step 1: load the dataset

```{r}
# libraries
library(tidyverse)
library(car)
library(ggstatsplot)
library(report)
```

```{r}
# load data
mpg_dta <- ggplot2::mpg |> 
  filter(cyl %in% c(4, 8)) |> 
  mutate(cyl = as.factor(cyl))

```

Step 2: Normality test

-   Ho: The data is normally distributed
-   Ha: The data is not normally distributed

Result: The data is not normally distributed Decision: Reject Ho and use a non-parametric tests (Mann-Whitney U test)

```{r}
# normality test
mpg_dta |> 
  group_by(cyl) |> 
  summarise(
    shapiro_test = shapiro.test(hwy)$statistic,
    p_value = shapiro.test(hwy)$p.value
  )
```

Step 3: test for homogeneity of variance

-   Ho: The data has equal variance
-   Ha: The data does not have equal variance

Decision: using 5% level of significance, we fail to reject the null hypothesis and conclude that the data has equal variance.

```{r}
# homogeneity of variance

leveneTest(data = mpg_dta, hwy ~ cyl)


```

Step 4: Independent t-test

Result: t = 17.186, df = 149, p-value \< 0.05

Decision rule: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the hwy miles per gallon (hwy) between 4-cylinder and 8-cylinder automobiles.

```{r}
# independent t-test
t.test(data = mpg_dta, hwy ~ cyl, var.equal = TRUE)

```

In case, variance are not equal, we can use the Welch's t-test.

```{r}
# independent t-test
t.test(data = mpg_dta, hwy ~ cyl, var.equal = FALSE)
```

Step 5: Visualization

```{r}
# visualization
ggbetweenstats(
  data = mpg_dta,
  x = cyl,
  y = hwy,
  title = "Highway miles per gallon by cylinder",
  xlab = "Cylinder",
  ylab = "Highway miles per gallon"
)

```

## Mann-whitney U test

Results: W = 5569.5, p-value \< 0.05

Decision rule: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the hwy miles per gallon (hwy) between 4-cylinder and 8-cylinder automobiles.

```{r}
# Mann-Whitney U test
wilcox.test(data = mpg_dta, hwy ~ cyl)
```

# Dependent sample t-test

Using the a synthetic data on training scores of before and after a training program, test whether there is a statistically significant difference in the scores before and after the training program.

Step 1: load the dataset

```{r}
## using a synthetic dataset
before <- c(12.2, 14.6, 13.4, 11.2, 12.7, 10.4, 15.8, 13.9, 9.5, 14.2)
after <- c(13.5, 15.2, 13.6, 12.8, 13.7, 11.3, 16.5, 13.4, 8.7, 14.6)

training_data <- tibble(subject = rep(c(1:10), 2), 
                   time = rep(c("before", "after"), each = 10),
                   score = c(before, after)) |> 
  mutate(time = as_factor(time))

training_data

```

Step 2: Normality test

-   Ho: The data is normally distributed
-   Ha: The data is not normally distributed

Results: before and after test scores have p-values \> 0.05. Decision: Fail to reject Ho and conclude that the data is normally distributed.

```{r}
# normality test
training_data |> 
  group_by(time) |> 
  summarise(
    shapiro_test = shapiro.test(score)$statistic,
    p_value = shapiro.test(score)$p.value
  )
```

Step 3: Homogeneity of variance

-   Ho: The data has equal variance
-   Ha: The data does not have equal variance

Results: p-value \> 0.05 Decision: Fail to reject Ho and conclude that the data has equal variance.

```{r}
# homogeneity of variance
leveneTest(data = training_data, score ~ time)
```

Step 4: Dependent sample t-test

Result: t = -2.272, p-value \< 0.05

Decision: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the scores before and after the training program.

```{r}
# dependent sample t-test
# t.test(data = training_data, score ~ time, paired = TRUE)

t.test(training_data$score[training_data$time == "before"],
       training_data$score[training_data$time == "after"],
       paired = TRUE)
```

Step 5: visualization

```{r}
# visualization
ggwithinstats(
  data = training_data,
  x = time,
  y = score,
  title = "Training scores before and after",
  xlab = "Time",
  ylab = "Scores"
)
```

# One-way ANOVA

```{r}
# load data
# Create the dataframe
exam_data <- data.frame(
  group = c(
    rep(1, 10),
    rep(2, 10),
    rep(3, 10)
  ),
  exam = c(
    50, 45, 48, 47, 45, 49, 50, 54, 57, 55, # Group 1
    63, 55, 54, 49, 65, 46, 53, 67, 58, 50, # Group 2
    71, 67, 68, 62, 65, 58, 63, 69, 70, 61  # Group 3
  )
) |> 
  mutate(group = as_factor(group))

exam_data
```

Step 2: Normality test

-   Ho: The data is normally distributed
-   Ha: The data is not normally distributed

Results: p-values \> 0.05

Decision: Fail to reject Ho and conclude that the data is normally distributed.

```{r}
# normality test
exam_data |> 
  group_by(group) |> 
  summarise(
    shapiro_test = shapiro.test(exam)$statistic,
    p_value = shapiro.test(exam)$p.value
  )
```

Step 3: Homogeneity of variance

-   Ho: The data has equal variance
-   Ha: The data does not have equal variance

Results: p-value \> 0.05

Decision: Fail to reject Ho and conclude that the data has equal variance.

```{r}
# homogeneity of variance
leveneTest(data = exam_data, exam ~ group)
```

Step 4: One-way ANOVA

Results: F = 21.01, p-value \< 0.05

Decision: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the exam scores between the three groups.

```{r}
# one-way ANOVA
aov_result <- aov(data = exam_data, exam ~ group)

summary(aov_result)
```

```{r}
report(aov_result)
```

Step 5: Post-hoc

```{r}
# post-hoc
posthoc <- TukeyHSD(aov_result)

posthoc
```

Step 6: Visualization

```{r}
# visualization
ggbetweenstats(
  data = exam_data,
  x = group,
  y = exam,
  title = "Exam scores by group",
  xlab = "Group",
  ylab = "Exam scores"
)

```

# Kruskal-Wallis

If data is not normally distributed, we can use the Kruskal-Wallis test.

Results: p-value \< 0.05

Decision: using 5% level of significance, we reject the null hypothesis and conclude that there is a statistically significant difference in the exam scores between the three groups.

```{r}
# Kruskal-Wallis test
kruskal.test(data = exam_data, exam ~ group)
```

## Post-hoc test

```{r}
# post-hoc
pairwise.wilcox.test(exam_data$exam, exam_data$group, p.adjust.method = "BH")
```

# Correlation

## Pearson correlation

Test if there is a relationship between mpg and car weight using mtcars dataset.

Step 1: load the data

```{r}
# load data
mtcars_data <- mtcars

mtcars_data
```

Step 2: Normality test

Results: both wt and mpg have p-values \> 0.05

Decision: Fail to reject Ho and conclude that the data is normally distributed.

```{r}
# normality test for mpg
shapiro.test(mtcars_data$mpg)

# normality test for wt
shapiro.test(mtcars_data$wt)
```

Step 3: Pearson correlation

Results: Correlation coefficient = -0.867, p-value \< 0.05

Interpretation: There is a strong negative correlation/association between mpg and wt. As the weight of the car increases, the miles per gallon decreases.

```{r}
# Pearson correlation
cor.test(mtcars_data$mpg, mtcars_data$wt, method = "pearson")
```

## Spearman correlation

Assuming the data is not normally distributed, we can use the Spearman correlation.

```{r}
# Spearman correlation
cor.test(mtcars_data$mpg, mtcars_data$wt, method = "spearman")
```

# Regression analysis

## Simple linear regression

```{r}
# load data

## import synthetic data
df <- tibble(
  Week = 1:15,
  Pie_Sales = c(350, 460, 350, 430, 350, 380, 430, 470, 450, 490, 340, 300, 440, 450, 300),
  Price = c(5.50, 7.50, 8.00, 8.00, 6.80, 7.50, 4.50, 6.40, 7.00, 5.00, 7.20, 7.90, 5.90, 5.00, 7.00),
  Advertising = c(3.3, 3.3, 3.0, 4.5, 3.0, 4.0, 3.0, 3.7, 3.5, 4.0, 3.5, 3.2, 4.0, 3.5, 2.7)
)

df

```

Model: Pie sales = a + b Price

$$
\text{pie sales} = \beta_0 - \beta_1 Price
$$

$$
\text{pie sales} = 558.28 - 24.03 Price
$$

Interpretation: for every 1 peso increase in Price, pie sales on average will decrease by 24.03 pies per week, holding other factors constant cet. par.

```{r}
# simple linear regression
lm(data = df, Pie_Sales ~ Price) |> summary()
```

## multiple linear regression

```{r}
# simple linear regression
lm(data = df, Pie_Sales ~ Price + Advertising) |> summary()
```
